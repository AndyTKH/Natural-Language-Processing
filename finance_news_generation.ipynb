{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial News Generation\n",
    "\n",
    "In this project, we will generate our own Financial News using RNNs.  We will be using Financial news published by Bloomberg.com, CNBC.com, reuters.com, wsj.com and fortune.com as our training dataset.  The Neural Network will then generate a new ,\"fake\" Financial News article, based on patterns it recognizes in this training dataset. Given an English sentence and the article length, the sample output of this project looks like below:\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "The data is available in the directory as `/data/fin_news.txt`, the text file contains all Financial news published by Bloomberg, CNBC, Reuters, Wsj and Fortune in January 2018. Below, we will create a `load_data` function to load in the data and output a sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Hours Ago | 02:56 \\nEmerging markets soared more than 33 percent in 2017, and Todd Gordon of TradingAnalysis.com says the rally won\\'t stop. \\nA big part of the rally in emerging markets, tracked by the emerging market ETF EEM , was a weak dollar. And given that Gordon still sees the inverse relationship between EEM and the dollar, measured in his charts by the dollar-tracking ETF UUP , he believes the U.S. currency will continue to help the group. \\n\"We have a falling U.S. dollar, which will support international and emerging market currencies and will give those EEM stocks a boost,\" Gordon said Tuesday on CNBC\\'s \"Trading Nation.\" The U.S. dollar in 2017 posted its worst annual performance in 14 years, while EEM saw its best performance since 2013. \\nAs for how high the latter could go, Gordon says EEM has broken \"resistance\" at around $45, which was the ETF\\'s 2014 highs. That $45 region is now what he calls \"support,\" and he sees it rallying to $50, which the ETF hasn\\'t hit s\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "data_dir = 'fin_news_2.txt'\n",
    "text = load_data(data_dir)\n",
    "print(text[2:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "\n",
    "### Lookup Table\n",
    "In dataset pre-processing, we will create a word embedding to transform the words to ids. The functions below will create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "The dictionaries will return **tuple** `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \n",
    "    # counter counts unique words and subsequently sort the words\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We will split the text into a word array using spaces as delimiters. However, punctuations like periods and comma can create multiple ids for the same word. For example, \"bullish\" and \"bullish!\" would generate two different word ids.\n",
    "\n",
    "The function `token_lookup` will return a dict that tokenize symbols like \"?\" into \"Question_Mark\".  The dictionary will have the symbol as key and the description as token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \n",
    "    # tokenize dictionary where the punctuation is the key, while the description is the token\n",
    "    punctuation = {'.':'<Period>', ',': '<Comma>', '\"': '<Quotation_Mark>', ';': '<Semicolon>', \n",
    "                   '!': '<Exclamation_Mark>', '?': '<Question_Mark>', '(': '<Left_Parentheses>', \n",
    "                   ')': '<Right_Parentheses>', '-': '<Dash>', '\\n': '<Return>'}\n",
    "    \n",
    "        \n",
    "    return punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "The code cell below will pre-process all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \n",
    "    # load text data\n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    # ignore brackets at the first and last 2 characters \n",
    "    text = text[2:-2]\n",
    "    \n",
    "    # pick only half of the data in January 2018 and preprocess the text\n",
    "    text = text[0:int(len(text)/2)] \n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # add delimiter (space) around the token  \n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "    \n",
    "    # split text into words\n",
    "    text = text.split()\n",
    "    \n",
    "    # create dictionaries for our vocabs and save it\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process training data\n",
    "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the vocabs\n",
    "The cell below will load the previously saved preprocessed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess(): \n",
    "    #Load the Preprocessed Training data\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190151"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "Build an RNN by implementing the RNN Module, forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Hence CPU is used to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "For input data, use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "Define TensorDataset to accept feature and target tensors. Then create a DataLoader that accepts the data output from TensorDataset and batch size.\n",
    "```\n",
    "data = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "### Batching\n",
    "Implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    "Create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`, then batch words using the DataLoader. \n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "int_words = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sequence_length = 5\n",
    "```\n",
    "\n",
    "Your first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4, 5]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "        \n",
    "    # only full batches\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for idx in range(0, (len(words) - sequence_length) ):\n",
    "        x.append(words[idx : idx + sequence_length])\n",
    "        y.append(words[idx + sequence_length])\n",
    "        \n",
    "    feature_tensors = torch.from_numpy(np.asarray(x))\n",
    "    target_tensors = torch.from_numpy(np.asarray(y))\n",
    "    data_output = TensorDataset(feature_tensors, target_tensors)\n",
    "    \n",
    "    # batch the neural network data using DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(data_output, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 1,  2,  3,  4,  5],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [ 3,  4,  5,  6,  7],\n",
      "        [ 4,  5,  6,  7,  8],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [ 7,  8,  9, 10, 11],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [ 9, 10, 11, 12, 13]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "test_text = range(50)\n",
    "train_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "data_iteration = iter(train_loader)\n",
    "x, y = data_iteration.next()\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print()\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "Implement an RNN using PyTorch's LSTM layers, or refer to Pytorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module) for other recurrent layers. The following functions are implemented for the class:\n",
    " - `__init__` - The initialization function\n",
    " - `init_hidden` - The initialization function for an LSTM hidden state\n",
    " - `forward` - Forward propagation function\n",
    " \n",
    "The initialization function creates the layers of the neural network and then save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "**The output of this model is the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # define linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "             \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # get batch size\n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # embedding and LSTM output\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack the outputs of the lstm to pass to the fully-connected layer \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # fully connected layers\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        # get the last batch of word scores by shaping the output of the final and fully-connected layer\n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        out = output.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # get last batch\n",
    "        out = out[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "            \n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "The function below will run the training loop iteratively, applying forward and back propagation on the RNN class, which will then return the average loss over a batch and the hidden state returned by the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    clip = 5\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # convert the initialized hidden state to tuple to feed into the model\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # zero out the accumulated gradient \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # get output and hidden from rnn model\n",
    "    output, h = rnn(inp, h)\n",
    "       \n",
    "    # calculate loss and perform backprop\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "  \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "The `train_rnn` function below will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    # switch model to train mode\n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "                \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss statistics\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "                save_model('./save/trained_rnn1', rnn)\n",
    "                print('Model Trained and Saved')\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence\n",
    "- Set `batch_size` to the batch size\n",
    "- Set `num_epochs` to the number of epochs to train for\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary\n",
    "- Set `output_size` to the number of output, which is the vocab_size\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN\n",
    "- Set `n_layers` to the number of layers/cells in your RNN\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in a sequence\n",
    "# size of a long range dependencies the model will learn\n",
    "sequence_length = 15  # of words in a sequence\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "\n",
    "# get train loader for the specific batch size and sequence length\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "# number of Epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# model parameters\n",
    "# vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "\n",
    "# output size\n",
    "output_size = 190151\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 500\n",
    "\n",
    "# LSTM hidden dimension\n",
    "hidden_dim = 1024\n",
    "\n",
    "# number of LSTM layers\n",
    "n_layers = 2\n",
    "\n",
    "# show statistics for every n number of batches\n",
    "show_every_n_batches = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Next, the neural network will begin training on the pre-processed data, and then save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "DELAY = INTERVAL = 4 * 60  # interval time in seconds\n",
    "MIN_DELAY = MIN_INTERVAL = 2 * 60\n",
    "KEEPALIVE_URL = \"https://nebula.udacity.com/api/v1/remote/keep-alive\"\n",
    "TOKEN_URL = \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\"\n",
    "TOKEN_HEADERS = {\"Metadata-Flavor\":\"Google\"}\n",
    "\n",
    "\n",
    "def _request_handler(headers):\n",
    "    def _handler(signum, frame):\n",
    "        requests.request(\"POST\", KEEPALIVE_URL, headers=headers)\n",
    "    return _handler\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def active_session(delay=DELAY, interval=INTERVAL):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "\n",
    "    from workspace_utils import active session\n",
    "\n",
    "    with active_session():\n",
    "        # do long-running work here\n",
    "    \"\"\"\n",
    "    token = requests.request(\"GET\", TOKEN_URL, headers=TOKEN_HEADERS).text\n",
    "    headers = {'Authorization': \"STAR \" + token}\n",
    "    delay = max(delay, MIN_DELAY)\n",
    "    interval = max(interval, MIN_INTERVAL)\n",
    "    original_handler = signal.getsignal(signal.SIGALRM)\n",
    "    try:\n",
    "        signal.signal(signal.SIGALRM, _request_handler(headers))\n",
    "        signal.setitimer(signal.ITIMER_REAL, delay, interval)\n",
    "        yield\n",
    "    finally:\n",
    "        signal.signal(signal.SIGALRM, original_handler)\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "\n",
    "\n",
    "def keep_awake(iterable, delay=DELAY, interval=INTERVAL):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "\n",
    "    from workspace_utils import keep_awake\n",
    "\n",
    "    for i in keep_awake(range(5)):\n",
    "        # do iteration with lots of work here\n",
    "    \"\"\"\n",
    "    with active_session(delay, interval): yield from iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 6.916114161968231\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 6.2781070510387424\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 6.219166939353943\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 6.010105189704895\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.879933169865608\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.857740352058411\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.833290056538582\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.74208405649662\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.747203724837303\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.83533194873333\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.6824871592760084\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.579512900781632\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.594374338531495\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.556385085725784\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.528165181708336\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.435497887063026\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.482322879695892\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.491042681467533\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.410441744756699\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.344062868070602\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.354757894086838\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.352898633420468\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.361197918248177\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.245707153403759\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.536586358608633\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.529946460914612\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.295924328374863\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.304437163734436\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.269421125483513\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.282022799539566\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.274082600545883\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.250532838892937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.290112085533142\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.4127979499340055\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.277865098690986\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.23269455139637\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.252567390620708\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.248969326388836\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.251597459363937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.172154213130474\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.236486044192314\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.250113327682018\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.196080506002903\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.1395271746039395\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.175180889570713\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.201294852077961\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.241532243931293\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.137922190892696\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.438127162766642\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.4837414794921875\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.201614346623421\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.213125530838966\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.1952378155350685\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.233290635871887\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.21247096130848\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.2072889700770375\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.254535237860679\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.390050026834011\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.252807159018516\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.216467375254631\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.217732063519954\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.239613149642945\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.227319112813473\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.14619226295948\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.216396837115288\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.239818999803067\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.195959291422367\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.153217913091183\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.195307377505302\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.229538003599644\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.281816409122944\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.187812846302986\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.463827360683432\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.513592452669144\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.197127249598503\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.258402144360542\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.250941630995274\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.2657275103569035\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.227526647591591\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.222591804862023\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.265731604707241\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.400036100912094\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.273551918625832\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.25086950494051\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.256910811042785\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.277697547328472\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.247948681998253\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.1755107597112655\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.266959977626801\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.276396996700764\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.225989170002937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.193787909042835\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.239581007432937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.276476565802097\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.369351597201824\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.268171687173844\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.532864142037953\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.5613490101337435\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.23488128939867\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.304182727646828\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.273913092243672\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.289374660527706\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.223370587944984\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.2356010409593585\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.276520078855753\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.408025255274772\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.2746079626798625\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.265591821503639\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.270127856969833\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.289662680840492\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.242030428314209\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.184495768034458\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.274660760211945\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.291992379283905\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.238845370483398\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.211137903273106\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.250892084538936\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.300207796895504\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.409793705403805\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.324661747193336\n",
      "\n",
      "Model Trained and Saved\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "#torch.backends.cudnn.enabled = False\n",
    "#with active_session():\n",
    "    \n",
    "    # create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "    #try:\n",
    "    #    rnn.cuda()\n",
    "    #except:\n",
    "    #    rnn.cuda()\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "    # defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "    # saving the trained model\n",
    "save_model('./save/trained_RNN', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I decide on the model hyperparameters? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** To reach for our goal of less than 3.5 in loss, we need to run a training of at least 10 epochs. Increasing the hidden dimension from 512 to 1024 has proven to converge faster with loss of 3 versus 2.5 after 10 epochs. The ideal n_layers was found to be 2. Increasing the n_layer to 3 or higher did not make the model converge faster, and it actually did worse with loss value oscillating between 6 and 7 even after 5 epochs. The ideal learn rate was found to be 0.001, if 0.01 learn rate was used, the gradient descent overshoots causing the loss to increase by each epochs. If a much lower 0.0001 was used in training, the converge would be too slow, hence very inefficient. Changing the embedding dimension from 500 to 800 did not make any significant improvement in the model, so we will stick to the lower 500 embedding dimension. Increasing the sequence length to 50 made the model to converge slower, and it took almost 4 hours to finish running just 1 epoch. In our case we will use the average English sentences length, which is 15 words per sentence, hence sequence length is set to 15.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load trained model\n",
    "\n",
    "After saving our trained model by name, `trained_RNN`, below cell will load in our word:id dictionaries and our trained RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "trained_rnn = torch.load('trained_RNN.pt', map_location=map_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Financial News\n",
    "With the network trained and saved, we can then generate a new, \"fake\" Financial News in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network will start predictions with the last single word of a user input sentence, and repeat its predictions until it reaches a set length. The `generate` function below will take a word id, `prime_id` to start with, and generates a set length of text, `predict_len`. Also, we use topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation descriptions\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    # swith model to evaluation mode\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "          \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "       \n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "       \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()  \n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    " \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(token, key)\n",
    "        \n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    #print(\"generated____________>\" ,gen_sentences)\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Financial News\n",
    "It's time to generate the text. Set `gen_length` to the length of Financial news article you want to generate and input sentence into `prime_sentence` to start the prediction.\n",
    "\n",
    "If the last word in your sentence cannot be recognized by vocab_to_int dictionary, the prediction will fail, please try again with other word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bullish will not be made only by means of an impasse . ” the commission said on friday it would take place in custody in favor of proposals to rein in the shortcomings , especially as an anti - eu single market conditions . eu officials said britain had said the arrest had received a divorce deal in october . “we will be able to hear a deal with eu member of eu members and a eu member of parliament’s economic policy , replacing an eu member of spain’s central banks at the annual economic forum meeting , brasilia said on saturday . the two - time tax was also agreed to be held on saturday after the first half of his three - year reign was sent to dump the olympic ice hockey tournament at melbourne park on saturday , handing a semi - finalist in the second set but beating a victory in the second set on saturday , beating her second match in court for chelsea in his second round encounter . “i was feeling mentally just two matches when we have played a medal match at madison in the match but was always very good for sure when he was very well at him , ” says he said in his speech . “so it’s a difficult , ” wenger said . “i am not my best , ” wenger said in his facebook post . “he did not play my game and mentally was very much i have to win myself . let’s have my goals . ” conte said she would have to have her encounter to play the encounter . “i am not going to be a good chance , “ conte said in an email to chelsea . “i am in a court in favour of myself i have no problem i have a chance , somebody has my life and mentally her when i have my career , “ team told his team . “so i was very seriously i have just heard my job in my life , “ conte told reuters at his palace . “i am not just maybe not a feeling , ” conte told reuters on saturday . soccer football - premier league skating match - chelsea vs brighton - atletico madrid , manchester city and southampton have won a pair of striker gael monfils in cardiff city in northern ireland . “i have a lot of guys who have so much i have to be honest when i would have to be honest when they do not leave your door to compete at the olympics . “i’ve seen the idea of sexual hotspur . ” conte was the first time at the olympic tournament in hospital , brighton , who said he had not won his match against manchester united was handed his match against manchester city at melbourne park in melbourne last year’s second round clash with manchester semi - finals over madison keys .\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 500 # modify the length to your preference\n",
    "prime_word = 'bullish' # name for starting the script\n",
    "\n",
    "#SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "pad_word = SPECIAL_WORDS['PADDING']\n",
    "generated_article = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your Financial News article\n",
    "\n",
    "Once you have an article that you like (or find interesting), the cell below will save the article to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_article.txt\",\"w\")\n",
    "f.write(generated_article)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Financial News is Not Perfect\n",
    "It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.\n",
    "\n",
    "You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

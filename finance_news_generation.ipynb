{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial News Generation\n",
    "\n",
    "In this project, we will generate our own Financial News using RNNs.  We will be using Financial news published by Bloomberg.com, CNBC.com, reuters.com, wsj.com and fortune.com as our training dataset.  The Neural Network will then generate a new ,\"fake\" Financial News article, based on patterns it recognizes in this training dataset. Given an English word  and the article length, the sample output of this project looks like below:\n",
    "\n",
    "<img src='image/news.png' width=100% />\n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "The data is available in the directory as `/data/fin_news.txt`, the text file contains all Financial news published by Bloomberg, CNBC, Reuters, Wsj and Fortune in January 2018. Below, we will create a `load_data` function to load in the data and output a sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Hours Ago | 02:56 \\nEmerging markets soared more than 33 percent in 2017, and Todd Gordon of TradingAnalysis.com says the rally won\\'t stop. \\nA big part of the rally in emerging markets, tracked by the emerging market ETF EEM , was a weak dollar. And given that Gordon still sees the inverse relationship between EEM and the dollar, measured in his charts by the dollar-tracking ETF UUP , he believes the U.S. currency will continue to help the group. \\n\"We have a falling U.S. dollar, which will support international and emerging market currencies and will give those EEM stocks a boost,\" Gordon said Tuesday on CNBC\\'s \"Trading Nation.\" The U.S. dollar in 2017 posted its worst annual performance in 14 years, while EEM saw its best performance since 2013. \\nAs for how high the latter could go, Gordon says EEM has broken \"resistance\" at around $45, which was the ETF\\'s 2014 highs. That $45 region is now what he calls \"support,\" and he sees it rallying to $50, which the ETF hasn\\'t hit s\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "data_dir = '/data/fin_news.txt'\n",
    "text = load_data(data_dir)\n",
    "print(text[2:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "\n",
    "### Lookup Table\n",
    "In dataset pre-processing, we will create a word embedding to transform the words to ids. The functions below will create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "The dictionaries will return **tuple** `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \n",
    "    # counter counts unique words and subsequently sort the words\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We will split the text into a word array using spaces as delimiters. However, punctuations like periods and comma can create multiple ids for the same word. For example, \"bullish\" and \"bullish!\" would generate two different word ids.\n",
    "\n",
    "The function `token_lookup` will return a dict that tokenize symbols like \"?\" into \"Question_Mark\".  The dictionary will have the symbol as key and the description as token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \n",
    "    # tokenize dictionary where the punctuation is the key, while the description is the token\n",
    "    punctuation = {'.':'<Period>', ',': '<Comma>', '\"': '<Quotation_Mark>', ';': '<Semicolon>', \n",
    "                   '!': '<Exclamation_Mark>', '?': '<Question_Mark>', '(': '<Left_Parentheses>', \n",
    "                   ')': '<Right_Parentheses>', '-': '<Dash>', '\\n': '<Return>'}\n",
    "    \n",
    "        \n",
    "    return punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "The code cell below will pre-process all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \n",
    "    # load text data\n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    # ignore brackets at the first and last 2 characters \n",
    "    text = text[2:-2]\n",
    "    \n",
    "    # pick only half of the data in January 2018 and preprocess the text\n",
    "    text = text[0:int(len(text)/2)] \n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # add delimiter (space) around the token  \n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "    \n",
    "    # split text into words\n",
    "    text = text.split()\n",
    "    \n",
    "    # create dictionaries for our vocabs and save it\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process training data\n",
    "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the vocabs\n",
    "The cell below will load the previously saved preprocessed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess(): \n",
    "    #Load the Preprocessed Training data\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190151"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "Build an RNN by implementing the RNN Module, forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Hence CPU is used to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "For input data, use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "Define TensorDataset to accept feature and target tensors. Then create a DataLoader that accepts the data output from TensorDataset and batch size.\n",
    "```\n",
    "data = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "### Batching\n",
    "Implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    "Create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`, then batch words using the DataLoader. \n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "int_words = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sequence_length = 5\n",
    "```\n",
    "\n",
    "Your first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4, 5]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "        \n",
    "    # only full batches\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for idx in range(0, (len(words) - sequence_length) ):\n",
    "        x.append(words[idx : idx + sequence_length])\n",
    "        y.append(words[idx + sequence_length])\n",
    "        \n",
    "    feature_tensors = torch.from_numpy(np.asarray(x))\n",
    "    target_tensors = torch.from_numpy(np.asarray(y))\n",
    "    data_output = TensorDataset(feature_tensors, target_tensors)\n",
    "    \n",
    "    # batch the neural network data using DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(data_output, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 1,  2,  3,  4,  5],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [ 3,  4,  5,  6,  7],\n",
      "        [ 4,  5,  6,  7,  8],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [ 7,  8,  9, 10, 11],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [ 9, 10, 11, 12, 13]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "test_text = range(50)\n",
    "train_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "data_iteration = iter(train_loader)\n",
    "x, y = data_iteration.next()\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print()\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "Implement an RNN using PyTorch's LSTM layers, or refer to Pytorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module) for other recurrent layers. The following functions are implemented for the class:\n",
    " - `__init__` - The initialization function\n",
    " - `init_hidden` - The initialization function for an LSTM hidden state\n",
    " - `forward` - Forward propagation function\n",
    " \n",
    "The initialization function creates the layers of the neural network and then save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "**The output of this model is the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # define linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "             \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # get batch size\n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # embedding and LSTM output\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack the outputs of the lstm to pass to the fully-connected layer \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # fully connected layers\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        # get the last batch of word scores by shaping the output of the final and fully-connected layer\n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        out = output.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # get last batch\n",
    "        out = out[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "            \n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "The function below will run the training loop iteratively, applying forward and back propagation on the RNN class, which will then return the average loss over a batch and the hidden state returned by the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    clip = 5\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # convert the initialized hidden state to tuple to feed into the model\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # zero out the accumulated gradient \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # get output and hidden from rnn model\n",
    "    output, h = rnn(inp, h)\n",
    "       \n",
    "    # calculate loss and perform backprop\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "  \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "The `train_rnn` function below will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    # switch model to train mode\n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "                \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss statistics\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "                save_model('./save/trained_rnn', rnn)\n",
    "                print('Model Trained and Saved')\n",
    "\n",
    "    # return a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence\n",
    "- Set `batch_size` to the batch size\n",
    "- Set `num_epochs` to the number of epochs to train for\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary\n",
    "- Set `output_size` to the number of output, which is the vocab_size\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN\n",
    "- Set `n_layers` to the number of layers/cells in your RNN\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in a sequence\n",
    "# size of a long range dependencies the model will learn\n",
    "sequence_length = 15  # of words in a sequence\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "\n",
    "# get train loader for the specific batch size and sequence length\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "# number of Epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# model parameters\n",
    "# vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "\n",
    "# output size\n",
    "output_size = vocab_size\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 500\n",
    "\n",
    "# LSTM hidden dimension\n",
    "hidden_dim = 1024\n",
    "\n",
    "# number of LSTM layers\n",
    "n_layers = 2\n",
    "\n",
    "# show statistics for every n number of batches\n",
    "show_every_n_batches = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Next, the neural network will begin training on the pre-processed data, and then save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 6.916114161968231\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 6.2781070510387424\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 6.219166939353943\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 6.010105189704895\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.879933169865608\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.857740352058411\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.833290056538582\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.74208405649662\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.747203724837303\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.83533194873333\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.6824871592760084\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.579512900781632\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.594374338531495\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.556385085725784\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.528165181708336\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.435497887063026\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.482322879695892\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.491042681467533\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.410441744756699\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.344062868070602\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.354757894086838\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.352898633420468\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.361197918248177\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    1/5     Loss: 5.245707153403759\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.536586358608633\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.529946460914612\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.295924328374863\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.304437163734436\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.269421125483513\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.282022799539566\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.274082600545883\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.250532838892937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.290112085533142\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.4127979499340055\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.277865098690986\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.23269455139637\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.252567390620708\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.248969326388836\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.251597459363937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.172154213130474\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.236486044192314\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.250113327682018\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.196080506002903\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.1395271746039395\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.175180889570713\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.201294852077961\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.241532243931293\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    2/5     Loss: 5.137922190892696\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.438127162766642\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.4837414794921875\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.201614346623421\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.213125530838966\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.1952378155350685\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.233290635871887\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.21247096130848\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.2072889700770375\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.254535237860679\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.390050026834011\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.252807159018516\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.216467375254631\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.217732063519954\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.239613149642945\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.227319112813473\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.14619226295948\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.216396837115288\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.239818999803067\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.195959291422367\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.153217913091183\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.195307377505302\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.229538003599644\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.281816409122944\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    3/5     Loss: 5.187812846302986\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.463827360683432\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.513592452669144\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.197127249598503\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.258402144360542\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.250941630995274\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.2657275103569035\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.227526647591591\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.222591804862023\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.265731604707241\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.400036100912094\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.273551918625832\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.25086950494051\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.256910811042785\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.277697547328472\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.247948681998253\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.1755107597112655\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.266959977626801\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.276396996700764\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.225989170002937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.193787909042835\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.239581007432937\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.276476565802097\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.369351597201824\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    4/5     Loss: 5.268171687173844\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.532864142037953\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.5613490101337435\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.23488128939867\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.304182727646828\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.273913092243672\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.289374660527706\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.223370587944984\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.2356010409593585\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.276520078855753\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.408025255274772\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.2746079626798625\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.265591821503639\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.270127856969833\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.289662680840492\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.242030428314209\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.184495768034458\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.274660760211945\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.291992379283905\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.238845370483398\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.211137903273106\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.250892084538936\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.300207796895504\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.409793705403805\n",
      "\n",
      "Model Trained and Saved\n",
      "Epoch:    5/5     Loss: 5.324661747193336\n",
      "\n",
      "Model Trained and Saved\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# define loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# save the trained model\n",
    "save_model('./save/final_trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I decide on the model hyperparameters? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach for our goal of less than 5.5 in loss, we run a training of at least 5 epochs. Increasing the hidden dimension from 512 to 1024 has proven to converge faster with loss of 5.8 versus 5.3 after 5 epochs. The ideal n_layers was found to be 2. Increasing the n_layer to 3 or higher did not make the model converge faster, and it actually did worse with loss value oscillating between 6 and 7 even after 5 epochs. The ideal learn rate was found to be 0.001, if 0.01 learn rate was used, the gradient descent overshoots causing the loss to increase by each epochs. If a much lower 0.0001 was used in training, the converge would be too slow, hence very inefficient. Changing the embedding dimension from 500 to 800 did not make any significant improvement in the model, so we will stick to the lower 500 embedding dimension. Increasing the sequence length to 50 made the model to converge slower, and it took almost 20 hours to finish running just 1 epoch. In our case we will use the average English sentences length, which is 15 words per sentence, hence sequence length is set to 15. \n",
    "\n",
    "The entire training took approximately 23 hours to finish runnign all 5 epochs, with the final loss recorded at 5.32. The training results show that the loss oscillates between 5.1 and 5.4 after 3 epochs, suggesting that a smaller learn rate may be required to further lower the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load trained model\n",
    "\n",
    "After saving our trained model by name, `trained_RNN`, below cell will load in our word:id dictionaries and our trained RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Hence CPU is used to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Hence CPU is used to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "trained_rnn = torch.load('final_trained_rnn.pt', map_location=map_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Financial News\n",
    "With the network trained and saved, we can then generate a new, \"fake\" Financial News in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network will start predictions with the last single word of a user input sentence, and repeat its predictions until it reaches a set length. The `generate` function below will take a word id, `prime_id` to start with, and generates a set length of text, `predict_len`. Also, we use topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation descriptions\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    # swith model to evaluation mode\n",
    "    rnn.eval()\n",
    "    \n",
    "    sequence_length = 15 \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "          \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "       \n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "       \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()  \n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    " \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle continues\n",
    "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    \n",
    "    gen_article = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_article = gen_article.replace(token, key)\n",
    "        \n",
    "    gen_article = gen_article.replace('\\n ', '\\n')\n",
    "    gen_article = gen_article.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article(word='banks', length=100):\n",
    "    news_article = generate(trained_rnn, vocab_to_int[word], int_to_vocab, token_dict, vocab_to_int['<PAD>'], length)\n",
    "    return news_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Financial News\n",
    "It's time to generate the text. Set `gen_length` to the length of Financial news article you want to generate and input sentence into `prime_sentence` to start the prediction.\n",
    "\n",
    "If the last word in your sentence cannot be recognized by vocab_to_int dictionary, the prediction will fail, please try again with other word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banks says in the eu will be aware to pay in favor . “for the first time , the commission has said they will take the podium . “i would like a trip to wedding and i’ve said myself was not at my time when i have to be honest , ” he said . “i’ve found my life , “ conte told the newspaper after the encounter was sent his wedding at an arena in sofia , peru , january 19 , 2018 . reuters/pierre gray/file halep had won a trip in favor of the dump in peru where they had two goals . slideshow (3 images ) “i am playing on myself i was very little to win at me . “i was very well when i was at the court when i was feeling out of him in the gym . “maybe , ” conte told reporters that the court was not on court documents , casting the conviction of wrongdoing . “i am not going back . ” conte will be angry in favor , ” conte said in a statement on friday that the court had received the arrest in favor of victims . weinstein has denied having not received his sentence in custody . “so we were surprised , ” conte said in a statement on twitter . “i have no comment . “maybe we cannot have to be sentenced because i have no plans to try if someone who have desperately listened if i have feeling him to work her life . “i have to play on him at my point at me , ” conte said . “i have to be honest at my life . “how i was not just a medal because i was not going my life , “\n"
     ]
    }
   ],
   "source": [
    "news_article = generate_article(word='banks', length=300)\n",
    "print(news_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bearish - life , twitter said on twitter that facebook had not yet have a warning on sexual harassment charges . weinstein said the lawsuit was “not harassed on the arrest of rape were not liable for him on the podium , said conte was not fired on saturday . the court had won a semi - final meeting with striker to play the case in the opening set on saturday but the swiss replied on a podium in madrid in slovenia , where he was defeated the dump in the dump . “i was the guy when i was going to be a great job . but i was a great match . i am not my best , “ conte said . “i was my life , i’ve seen myself i don’t have the feeling , “ conte told his court in brooklyn , chelsea , who had had to be handed over madrid in prison in hospital in manchester , britain , january 19 , 2018 , juan barros , who was ousted in custody after the rivalry . slideshow (2 images ) the suspect was accused of extremist and fired by the colombian of the police , dressed in prison in the neighbouring state and said on twitter that was arrested in custody in prison on saturday . authorities have said they were arrested on saturday , saying his spokesman was “not not a “good police on twitter at a later stage . “he said the case would be a case in court for me to try to make up to the victims , ” conte said in a speech . “when we have to be arrested by him in court for me and we have not had to correct your job . ”reporting by alan baldwin ; editing by jason neely' , 'beirut (reuters ) - turkish president tayyip erdogan said on saturday that turkey had deployed a migrant on twitter during a visit to lima on saturday . turkey has fired two charges over the arrest . authorities on saturday and the arrest of the suspect , saying that the trip had been tested as part because of the rivalry . the striker has won two goals for his first grand slam final at crystal palace in manchester city in manchester on saturday . soccer football - premier league - brighton stadium , manchester city arena , melbourne park - melbourne park semi - finalist konta - brighton , brighton - czech arena defeated brighton - manchester city stadium v brighton arena win over manchester city of sofia , brighton in sofia city of chelsea , manchester city and brighton said they had not received the arrest in bern after the arrest . chelsea denied any suggestion was unclear when they had two free skate at midnight in custody and was sent his team at sofia in front of vernon in the women’s ice hockey league since his former slalom semi - finalist\n"
     ]
    }
   ],
   "source": [
    "news_article = generate_article(word='bearish', length=500)\n",
    "print(news_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your Financial News article\n",
    "\n",
    "Once you have an article that you like (or find interesting), the cell below will save the article to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generated financial news to a text file\n",
    "f =  open(\"financial_news.txt\",\"w\")\n",
    "f.write(news_article)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Financial News is Not Perfect\n",
    "Based on the generated financial news above, the contents of the article may not make perfect sense but the model has certainly learned to produce complete sentences. For further improvement in the model, a smaller learn rate (0.0003, 0.0002, 0.0001) and larger hidden dimension (2048, 4096) should be used for training. Additionally, more financial news training sample should also be included."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
